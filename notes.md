## Mechanistic Exploration of Gemma 2 2b list creation


### 1. Introduction
Gemma 2 2b is a Small Language Model, created by google that was made public in Summer 2024. This model was released along a suite of Sparse Autoenocders and Transcoders trained on their activations in various locations.


This model, despite it's size is incredible capable, excelling in it's instruction tuned vairant with the ability to follow instuctions and with similar performance to the original GPT-3 model on some benchmarks.


This facts make Gemma 2 2b a great candidate to perform MI experiments on.

In this post we will explore the mechanisms behind Gemma 2 2b's ability to create lists of items, when prompted for.

Specially we are interested in the mechanism by which Gemma knows when to end a list, this is task is interesting for the following reasons:

- Due to the larger Gemma 2 vocabulary is eassy to create one token per item list templaets.
- The instruction tuning of the model, enables the induction of different behaviors in the model with minimal 
- The open endedness of this task enables taking into account sampling dynamics on the decoding process of the model (this is temperature and sampling method).
- The temmplate structure enables a clear analysis of a priori very broad properties of the model like "list ending behavior" by proxies such as the probability of outputing a hypen after a list item which clearly indicates that the list is about to continue.



### 2.Data

To investigate the behavior of Gemma when asked for a list we create a synthetic dataset.


1) We ask GPT4-o to provide a list of topics to create lists about.
2) we create a prompt, for Gemma:
3) For each topic, we sample 10 Gemma Completions with top-k = 0.9 and temeprature=0.8

```
Provide me  a with short list of a few {topic}. Just provide the names, no need for any other information.

```
*This step is crucial, because as can be seen in the Apendix actually sampling completions from Gemma allow us to observe the actual dynamics of the list ending behavior when compared with List Gnerated by GPT4o*

4) Finally for all the completions generated by Gemma, we construct contrastive prompts.

```
Provide me with a long list of a many {topic}. Just provide the names, no need for any other information.
```
This contrastive pairs are shown to not have the same list ending behaviors which enable patching style Interpretabiliy without too much shift in behavior unrelated activations.



### 3. Exploratory Analysis

We start the Analysis with an exploratory overview of the completions provided by the model.
Few interesting things that were interesting:
1) Most of the item's in the list where a single token.
2) For all the topics, in the last few items in the list the model sampled white space tokens after the item and before the line break.
3) The number of items in each list with a white space added is pretty consistent across topics, with a few outliers.
4) The number of items in each list is also very similar across topics.
5) There exist a correlation between the sampling temperature and the number of items in a list with a blank spaces token before the end of the list.
6) For prompts, where we asked for a long list, the average number of items is 30, and we no longer observe an abudance of white space tokens at the end of the list.



### 4. Logit Lens


Taking advantage of the shared RS across layers we can use the unembedding matrix to project into vocabulary space activations across the layers, to get an intuition of how a behavior builds trought the layers.



Using such techniques we inspect the relevant positions for the list ending behavior across the dataset.

We use the difference between decoder <end_of_turn> and "-" direction as the list ending directio, to inspect the activations trough the layers.

This important locations are the various positions in which the list could have ended, this is the line_break positions.

Inspecting the different location Logit Lens, we see a shared trend of increasing logit_difference across later items in the list and layers.

Being layer 18 to 26 and the list items with white space tokens the breaking points for the most difference in logit differnce layer and item wise.









### 5. Activation Patching

/ie of the nices properties of this setup is that due to the instruction tuning of the model is very easy to induces certain model behaviors with minimal changes in the prompt.


In this case just changing the tokens "short list with a few" with "long list with a many" make the model output much longer list, and hence display different list ending behaviors for a given prompt.


This enables easy creation of the contrastive templates.



With this easy trick we are in an very similar situation (while not perfectly analogous ) to the IOI paper.

This enable simple patching experiments to identify crucial model components for the list ending behavior to occur.

- Patchings:
    - Residual stream
    - Key, Value, Query
    - MLP output
    - Attention score
- Edge patching:




### 6. Feature Attribution

To bound the complexity of using Sparse Autoencoders, to inestigate the model behavior we will focus on residual stream features.

One problem that is apparent to anyone that has tried to use SparseAutoencoders for real world task is that the memory footprint of SAE experiments rapidly explodes as we add layers.

The intuitive solution to this problem is to come up with heuristics to select a few layers to use SAEs in, to maximize the faithfulness/GB vram ratio.


- Attribution wrt to the logit differnce
- Attribution wrt to the -log prob
- Feature attribution (most important later layer feaurtes) (If I have enough time)




### 7. Feature visualization in the dataset

- For each topic select a number of features across the layers, that when ablated modifies the list ending behavior.
- Display the activtion of this features across the dataset, include contrasitve pairs.
- How much overlap is there between topics



### 8. Causal ablation of RS features over the layers


### Appendix


**Item break vs White space Locations**

**Should we divide Logit Lens by temperature**

**Heuristic for selecting the layers**


