{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import HookedSAETransformer, SAE, SAEConfig\n",
    "from gemma_utils import get_gemma_2_config, gemma_2_sae_loader, get_all_string_min_l0_resid_gemma\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import einops\n",
    "import re\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Any\n",
    "from torch import Tensor\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72c3fc261210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2286f24694a47819ba57ea762294a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedSAETransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-25): 26 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln1_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b-it\", device = \"cpu\")\n",
    "#model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"santhoshmlops/Skai_Gemma_Instruct_ChatTemplate\",split = \"train\")\n",
    "text = ds[:1000]['text']\n",
    "tokenized_ds = model.tokenizer(text, return_tensors = \"pt\", padding = True)\n",
    "pairs_length = tokenized_ds['attention_mask'].sum(dim = -1).numpy()\n",
    "indices = torch.where(torch.tensor(np.logical_and(pairs_length>50, pairs_length<200)))\n",
    "text_array = np.array(text)[indices]\n",
    "samp_ind = torch.randperm(len(text_array))[:300]\n",
    "final_text = text_array[samp_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab18c20bbfb4e18a7656c4eb4aa9eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    106,   1645,  30641,    603,    671,  14239,    674,  19306,\n",
      "            476,   6911, 235265,  15615,    476,   3590,    674,  47258,  56382,\n",
      "            573,   3853, 235265,    109,  47239,    919, 235248, 235284,  53190,\n",
      "         235269, 235248, 235304,  35768,    578, 235248, 235284,  83699, 235265,\n",
      "          17487,    919, 235248, 235318,  63495, 235269, 235248, 235274, 235284,\n",
      "          41612,    578, 235248, 235284, 108160, 235265,  27161,    919, 235248,\n",
      "         235274,   1226, 235269, 235248, 235274, 235276,  35768,    578, 235248,\n",
      "         235318,  64322, 235265,   2250,   1767,   7091,   8398,    749,  47239,\n",
      "         235269,  17487,    578,  27161,    791,    575,   3051, 235336, 235248,\n",
      "            107,    108,    106,   2516, 235248,    108, 227128, 235269,  35768,\n",
      "         235269,  64322,    578,  41612,    708,  17203,   5604,    577,    614,\n",
      "           7091,   8398,   1861,    984,    708,  17203,   9244,    575,  26511,\n",
      "            578,    708,  13968,    604,   3515,  14072, 235265,  35682, 235269,\n",
      "          63495,    708,    780,   5604,    577,    614,   7091,   8398,   1861,\n",
      "            984,   3695,   2664,    908,    575,    573,   7571,    578,    708,\n",
      "            780,   3695,  33561,    731,  17611, 235265,  26487, 235269,  83699,\n",
      "            578, 108160,    708,    780,   5027,   2652,    712,    984,    708,\n",
      "            780,   7091,   8398, 235265,  47239, 235269,  17487,    578,  27161,\n",
      "            791, 235248, 235284,    963, 235248, 235304,    963, 235248, 235274,\n",
      "         235284,    963, 235248, 235274, 235276,    963, 235248, 235318,    589,\n",
      "         235248, 235304, 235304,   7091,   8398,    575,   3051, 235265, 235248,\n",
      "            107,    106,   2516, 235248,    109,   4858, 235303, 235256,   1368,\n",
      "            577,  13988,    573,   3051,   1758,    576,   7091,   8398, 235292,\n",
      "            109, 235287,   5231,  79762,  66058, 235248, 235284,  53190,    963,\n",
      "         235248, 235304,  35768,    589, 235248, 235308,    108, 235287,   5231,\n",
      "          30607,  66058, 235248, 235318,  63495,    963, 235248, 235274, 235284,\n",
      "          41612,    963, 235248, 235284, 108160,    589, 235248, 235284, 235276,\n",
      "            108, 235287,   5231,  29613,  66058, 235248, 235274,   1226,    963,\n",
      "         235248, 235274, 235276,  35768,    963, 235248, 235318,  64322,    589,\n",
      "         235248, 235274, 235324,    109,    688,   6457,  66058, 235248, 235308,\n",
      "            963, 235248, 235284, 235276,    963, 235248, 235274, 235324,    589,\n",
      "           5231, 235310, 235284,    688,    109,  79762, 235269,  17487, 235269,\n",
      "            578,  27161,    791,    476,   3051,    576, 235248, 235310, 235284,\n",
      "           7091,   8398, 235265, 235248,    108,    107]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = final_text[0]\n",
    "text = text.split(\"<statr_of_turn>model\")[0]+\"<start_of_turn>model\"\n",
    "tokens = model.to_tokens(text, prepend_bos = False)\n",
    "with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens = 400,\n",
    "            temperature = 0.7,\n",
    "            top_p = 0.9,\n",
    "            eos_token_id=107,\n",
    "            stop_at_eos=True,\n",
    "            )\n",
    "        \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from attribution_utils import calculate_feature_attribution \n",
    "from torch.nn.functional import log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metric_fn(logits: torch.Tensor) -> torch.Tensor:\n",
    "    log_probs = log_softmax(logits, dim = -1)\n",
    "    #x = - torch.gather(log_probs[:,:-1,:],-1, tokens[:,1:].unsqueeze(-1))\n",
    "    x = log_probs[:,-1,107]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GemmaExploration/gemma_utils.py:158: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  resid_dict = df[df['release'] == \"gemma-scope-2b-pt-res\"]['saes_map'][0]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_strings= get_all_string_min_l0_resid_gemma()\n",
    "saes_dict = {}\n",
    "with torch.no_grad():\n",
    "    for layer in tqdm.tqdm([22,23]):\n",
    "\n",
    "        repo_id = \"google/gemma-scope-2b-pt-res\"\n",
    "        folder_name = full_strings[layer]\n",
    "\n",
    "        config = get_gemma_2_config(repo_id, folder_name)\n",
    "        cfg, state_dict, log_spar = gemma_2_sae_loader(repo_id, folder_name)\n",
    "        sae_cfg = SAEConfig.from_dict(cfg)\n",
    "        sae = SAE(sae_cfg)\n",
    "        sae.load_state_dict(state_dict)\n",
    "        #sae.to(\"cuda:0\")\n",
    "        saes_dict[sae.cfg.hook_name] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 66.94 MiB is free. Process 711445 has 23.59 GiB memory in use. Of the allocated memory 22.83 GiB is allocated by PyTorch, and 463.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(out\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 2\u001b[0m feature_attribution_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_feature_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_saes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaes_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_error_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GemmaExploration/attribution_utils.py:181\u001b[0m, in \u001b[0;36mcalculate_feature_attribution\u001b[0;34m(model, input, metric_fn, track_hook_points, include_saes, return_logits, include_error_term)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_feature_attribution\u001b[39m(\n\u001b[1;32m    172\u001b[0m     model: HookedSAETransformer,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28minput\u001b[39m: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     include_error_term: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Attribution:\n\u001b[0;32m--> 181\u001b[0m     outputs_with_grads \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_attribution_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrack_hook_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_saes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_saes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_error_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_error_term\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     model_attributions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    191\u001b[0m     model_activations \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/GemmaExploration/attribution_utils.py:162\u001b[0m, in \u001b[0;36mcalculate_attribution_grads\u001b[0;34m(model, prompt, metric_fn, track_hook_points, include_saes, return_logits, include_error_term)\u001b[0m\n\u001b[1;32m    160\u001b[0m gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(metric)\n\u001b[1;32m    161\u001b[0m output\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 162\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AttributionGrads(\n\u001b[1;32m    164\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    165\u001b[0m     model_output\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mmodel_output,\n\u001b[1;32m    166\u001b[0m     model_activations\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mmodel_activations,\n\u001b[1;32m    167\u001b[0m     sae_activations\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39msae_activations,\n\u001b[1;32m    168\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 66.94 MiB is free. Process 711445 has 23.59 GiB memory in use. Of the allocated memory 22.83 GiB is allocated by PyTorch, and 463.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = torch.tensor(out.tolist())\n",
    "feature_attribution_df = calculate_feature_attribution(\n",
    "    model = model,\n",
    "    input = tokens,\n",
    "    metric_fn = metric_fn,\n",
    "    include_saes=saes_dict,\n",
    "    include_error_term=True,\n",
    "    return_logits=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
