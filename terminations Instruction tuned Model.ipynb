{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false},"outputs":[],"source":["\n","from sae_lens import HookedSAETransformer, SAE, SAEConfig\n","from gemma_utils import get_gemma_2_config, gemma_2_sae_loader\n"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","from sae_lens import HookedSAETransformer, SAE, SAEConfig\n","from gemma_utils import get_gemma_2_config, gemma_2_sae_loader\n","import numpy as np\n","import torch\n","import tqdm\n","import einops\n","import re\n","from jaxtyping import Int, Float\n","from typing import List, Optional, Any\n","from torch import Tensor\n","import json\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","import random\n","from transformer_lens.utils import get_act_name\n","from IPython.display import display, HTML\n","import plotly.express as px\n"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f063db3b9f0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\n","np.random.seed(0)\n","random.seed(0)\n","torch.random.manual_seed(0)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"254d16c8117943969e66cc989c1c61a7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"]},{"name":"stdout","output_type":"stream","text":["Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n"]}],"source":["\n","model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b-it\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[],"source":["topics = [\"City Names\",\"Countries\",\"Animals\",\"Types of Trees\",\"Types of Flowers\",\n","\"Fruits\",\"Vegetables\",\"Car Brands\",\"Sports\",\"Rivers\",\"Mountains\",\"Ocean\",\n","\"Inventions\",\"Languages\",\"Capital Cities\",\"Movies\",\"Books\",\"TV Shows\",\n","\"Famous Scientists\",\"Famous Writers\",\"Video Games\",\"Companies\",\"Colors\"]\n"]},{"cell_type":"raw","metadata":{"collapsed":false},"source":["topic = \"english girl names\"\n"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[],"source":["def generate_lists(topics):\n","    generation_dict = {}\n","    for topic in topics:\n","        generation_dict[topic] = []\n","        for _ in range(5):\n","            messages = [\n","                {\"role\": \"user\", \"content\": f\"Provide me with a short list of a few {topic}. Just provide the names, no need for any other information.\"},\n","            ]\n","            input_ids = model.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","            input_ids += \"-\"\n","            tokens = model.to_tokens(input_ids, prepend_bos=False)\n","            out = model.generate(\n","                tokens,\n","                max_new_tokens = 200,\n","                temperature = 0.7,\n","                top_p = 0.8,\n","                stop_at_eos=True,\n","                )\n","\n","            toks = out.detach().clone()\n","            generation_dict[topic].append(toks)\n","    return generation_dict\n","#generation_dict = generate_lists(topics)\n","#torch.save(generation_dict, \"gemma2_generation_dict.pt\")\n","generation_dict = torch.load(\"gemma2_generation_dict.pt\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","\n","\n","hypen_tok_id = 235290\n","break_tok_id = 108\n","eot_tok_id = 107\n","blanck_tok_id = 235248\n","toks = generation_dict[\"City Names\"][0].squeeze()\n","hypen_positions = torch.where(toks == hypen_tok_id)[0].to(\"cpu\")\n","print(hypen_positions)\n","break_positions = torch.where(toks == break_tok_id)[0].to(\"cpu\")\n","print(break_positions)\n","eot_positions = torch.where(toks == eot_tok_id)[0].to(\"cpu\")\n","print(eot_positions)\n","filter_break_pos = [pos.item() for pos in break_positions if pos+1 in hypen_positions]\n","topic_spans = [(hypen_positions[i].item(),hypen_positions[i+1].item()) for i in range(len(hypen_positions)-1)] +[(hypen_positions[-1].item(),eot_positions[-1].item())]\n","token_spans = []\n","for span in topic_spans:\n","    token_spans.append(toks[span[0]:span[1]].tolist())\n","\n","print(len(token_spans))\n","number_of_tokens_per_item = [len(span) for span in token_spans]\n","print(number_of_tokens_per_item)\n","white_space_tok = torch.tensor([235248 in tok_span for tok_span in token_spans])\n","white_spaces_tok_pos = torch.where(white_space_tok)[0].to(\"cpu\")\n","print(white_space_tok)\n","print(white_spaces_tok_pos)\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### What's happening with whitespaces\n"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false},"outputs":[],"source":["def get_stats(dict_toks):\n","    \"\"\"\n","    For each topic create a dictionary of stats\n","    For each generated list get:\n","    - Number of tokens\n","    - Number of items in the list\n","    - Average number of tokens per item\n","    - Item positions in which blank tokens are foung\n","    \"\"\"\n","    stats_dict = {}\n","    for topic, toks_list in dict_toks.items():\n","        stats_dict[topic] = []\n","        for toks in toks_list:\n","            toks = toks.squeeze()\n","\n","            hypen_positions = torch.where(toks == hypen_tok_id)[0].to(\"cpu\")\n","            break_positions = torch.where(toks == break_tok_id)[0].to(\"cpu\")\n","            eot_positions = torch.where(toks == eot_tok_id)[0].to(\"cpu\")\n","            filter_break_pos = [pos.item() for pos in break_positions if pos+1 in hypen_positions]\n","            topic_spans = [(hypen_positions[i].item(),hypen_positions[i+1].item()) for i in range(len(hypen_positions)-1)] +[(hypen_positions[-1].item(),eot_positions[-1].item())]\n","            token_spans = []\n","            for span in topic_spans:\n","                token_spans.append(toks[span[0]:span[1]].tolist())\n","            num_items = len(token_spans)\n","            number_of_tokens_per_item = torch.tensor([len(span) for span in token_spans])\n","            white_space_tok = torch.tensor([235248 in tok_span for tok_span in token_spans])\n","            white_spaces_tok_pos = torch.where(white_space_tok)[0].to(\"cpu\")\n","\n","            stats_dict[topic].append({\"num_tokens\": number_of_tokens_per_item, \"num_items\": num_items, \"avg_tokens_per_item\": number_of_tokens_per_item, \"blank_positions\": white_spaces_tok_pos})\n","    return stats_dict\n","\n","\n","\n","\n","stats_dict = get_stats(generation_dict)\n","    \n"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false},"outputs":[],"source":["import plotly.graph_objects as go\n","import torch\n","\n","def plot_stats(stats_dict):\n","    \"\"\"\n","    Plot the contents of the stats_dict generated from get_stats in Plotly interactive plots.\n","    \"\"\"\n","    # Create separate lists for each stat\n","    topics = []\n","    num_tokens_per_topic = []\n","    avg_tokens_per_topic = []\n","    num_items_per_topic = []\n","    \n","    num_blank_tokens_per_topic = []\n","    blank_token_positions = []\n","\n","    for topic, stats_list in stats_dict.items():\n","        # Iterate through the lists for each topic\n","        for stats in stats_list:\n","            topics.append(topic)\n","            num_tokens_per_topic.append(stats['num_tokens'].sum().item()/len(stats[\"num_tokens\"]))  # Total number of tokens\n","            avg_tokens_per_topic.append(stats['avg_tokens_per_item'].float().mean().item())  # Average tokens per item\n","            num_items_per_topic.append(stats['num_items'])  # Number of items\n","            # Number of blank tokens\n","            num_blank_tokens_per_topic.append(len(stats['blank_positions']))\n","            \n","            # Blank token positions (we need to record topic and position)\n","            blank_token_positions.extend([(topic, pos.item()) for pos in stats['blank_positions']])\n","    # Create the interactive plot with Plotly\n","    fig = go.Figure()\n","\n","    # Adding a bar chart for the number of tokens per topic\n","    fig.add_trace(go.Bar(\n","        x=topics,\n","        y=num_tokens_per_topic,\n","        name='Number of Tokens',\n","        marker_color='blue'\n","    ))\n","\n","    # Adding a line chart for average tokens per item\n","    fig.add_trace(go.Scatter(\n","        x=topics,\n","        y=avg_tokens_per_topic,\n","        name='Average Tokens per Item',\n","        mode='lines+markers',\n","        marker_color='green'\n","    ))\n","\n","    # Adding a bar chart for the number of items per topic\n","    fig.add_trace(go.Bar(\n","        x=topics,\n","        y=num_items_per_topic,\n","        name='Number of Items',\n","        marker_color='orange',\n","        opacity=0.6\n","    ))\n","\n","    # Update the layout for better presentation\n","    fig.update_layout(\n","        title=\"Token Statistics per Topic\",\n","        xaxis_title=\"Topics\",\n","        yaxis_title=\"Values\",\n","        barmode='group',  # Bars will be shown side by side\n","        legend=dict(\n","            x=0.1,\n","            y=1.1,\n","            orientation=\"h\"\n","        ),\n","        template='plotly_dark'\n","    )\n","\n","    fig1 = go.Figure()\n","    \n","    fig1.add_trace(go.Scatter(\n","        x=topics,\n","        y=avg_tokens_per_topic,\n","        mode='lines+markers',\n","        name='Avg Tokens per Item',\n","        marker=dict(color='green', size=8),\n","        line=dict(color='green', width=2)\n","    ))\n","\n","    fig1.update_layout(\n","        title=\"Average Tokens per Item across Topics\",\n","        xaxis_title=\"Topics\",\n","        yaxis_title=\"Average Number of Tokens\",\n","        template=\"plotly_dark\"\n","    )\n","\n","    # Plot 2: Bar plot for number of blank tokens per topic\n","    fig2 = go.Figure()\n","\n","    fig2.add_trace(go.Bar(\n","        x=topics,\n","        y=num_blank_tokens_per_topic,\n","        name='Number of Blank Tokens',\n","        marker_color='orange',\n","        opacity=0.7\n","    ))\n","\n","    # Adding a scatter plot for blank token positions\n","    if blank_token_positions:\n","        topics_pos, positions = zip(*blank_token_positions)  # Unzipping topic and position data\n","        fig2.add_trace(go.Scatter(\n","            x=topics_pos,\n","            y=positions,\n","            mode='markers',\n","            name='Blank Token Positions',\n","            marker=dict(color='red', size=8, symbol='x')\n","        ))\n","\n","    fig2.update_layout(\n","        title=\"Blank Token Information across Topics\",\n","        xaxis_title=\"Topics\",\n","        yaxis_title=\"Blank Token Count / Position\",\n","        template=\"plotly_dark\",\n","        showlegend=True\n","    )\n","    # Show the plot\n","    fig.show()\n","\n","    fig1.show()\n","    fig2.show()\n","\n","plot_stats(stats_dict)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false},"outputs":[],"source":["import plotly.graph_objects as go\n","import torch\n","\n","def plot_items_vs_blank_tokens(stats_dict):\n","    \"\"\"\n","    Plot a direct comparison between the number of items per topic and the number of blank tokens per topic.\n","    \"\"\"\n","    # Create separate lists for each stat\n","    topics = []\n","    num_items_per_topic = []\n","    num_blank_tokens_per_topic = []\n","    proportion_blank_tokens_per_topic = []\n","\n","    for topic, stats_list in stats_dict.items():\n","        for stats in stats_list:\n","            topics.append(topic)\n","            \n","            # Number of items per topic\n","            num_items_per_topic.append(stats['num_items'])\n","            \n","            # Number of blank tokens per topic\n","            num_blank_tokens_per_topic.append(len(stats['blank_positions']))\n","            # Proportion of blank tokens per topic\n","            proportion_blank_tokens_per_topic.append(len(stats['blank_positions']) / len(stats['num_tokens']))\n","\n","    # Create the grouped bar chart\n","    fig = go.Figure()\n","\n","    # Bar for the proportion of blank tokens per topic \n","    fig.add_trace(go.Box(\n","        x=topics,\n","        y=proportion_blank_tokens_per_topic,\n","        name='Proportion of Blank Tokens',\n","        marker_color='blue'\n","    ))\n","\n","    # Update the layout for better visualization\n","    fig.update_layout(\n","        title=\"Comparison of Number of Items vs. Blank Tokens per Topic\",\n","        xaxis_title=\"Topics\",\n","        yaxis_title=\"Count\",\n","        barmode='group',  # Bars are grouped side by side for comparison\n","        template='plotly_dark',\n","        legend=dict(x=0.1, y=1.1, orientation=\"h\")\n","    )\n","\n","    # Display the plot\n","    fig.show()\n","\n","# Example usage (you would call get_stats first and pass the result to plot_items_vs_blank_tokens)\n","# dict_toks = {...}  # Input dictionary of tokens per topic\n","# stats = get_stats(dict_toks)\n","plot_items_vs_blank_tokens(stats_dict)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Template\n","\n","- Create Template\n","- Sample from the Template \n","- Filtering with final logits\n"]},{"cell_type":"code","execution_count":27,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","toks = generation_dict[\"Vegetables\"][0]\n","#with torch.no_grad():\n","#    logits,cache = model.run_with_cache(toks)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false},"outputs":[],"source":["all_logit_diff = logits[:,:,eot_tok_id] - logits[:,:,hypen_tok_id]\n","all_logit_diff = all_logit_diff.cpu().squeeze().numpy()\n","\n","\n","\n","\n","# Display the difference in logit diff over the positions with a lineplot\n","# Use the tokens as the x\n","str_tokens = model.to_str_tokens(toks[0])\n","\n","unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(str_tokens)]\n","\n","\n","fig = px.bar(\n","    x=unique_tokens,\n","    y=all_logit_diff,\n","    labels={'x': 'Unique Tokens', 'y': 'Logit Difference'},  # Axis labels\n","    title=\"Logit Differences Across Unique Tokens\"  # Plot title\n",")\n","\n","# Customize the layout to add titles, axis labels, and other styling\n","fig.update_layout(\n","    title={\n","        'text': \"Logit Differences Across Unique Tokens\",\n","        'x': 0.5,  # Center the title\n","        'xanchor': 'center',\n","        'yanchor': 'top'\n","    },\n","    xaxis_title=\"Unique Tokens\",\n","    yaxis_title=\"Logit Difference\",\n","    xaxis_tickangle=-45,  # Rotate x-axis labels for better readability\n","    legend_title=\"Legend Title\",  # Add a title to the legend if multiple traces\n","    showlegend=True  # Ensure the legend is shown (not necessary for single trace)\n",")\n","\n","# Show the plot\n","fig.show()\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### Create Template with the same structure \n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Logit Lens\n"]},{"cell_type":"code","execution_count":28,"metadata":{"collapsed":false},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","def plot_superposed_lineplots(data, x, y, hue, title=None, xlabel=None, ylabel=None, \n","                              palette='husl', line_styles=None, markers=True, \n","                              grid=True, legend=True, legend_loc='best', \n","                              figsize=(10, 6), linewidth=2):\n","    \"\"\"\n","    Plots multiple superposed line plots with custom aesthetics using Seaborn.\n","\n","    Parameters:\n","        data (pd.DataFrame): DataFrame containing the data.\n","        x (str): Column name to be used for the x-axis.\n","        y (str): Column name to be used for the y-axis.\n","        hue (str): Column name to be used for grouping data (each group gets a different line).\n","        title (str): Title of the plot.\n","        xlabel (str): Label for the x-axis.\n","        ylabel (str): Label for the y-axis.\n","        palette (str or list): Seaborn color palette or list of colors.\n","        line_styles (list or dict): List or dictionary of line styles for each hue category.\n","        markers (bool): Whether to show markers at data points.\n","        grid (bool): Whether to show grid lines.\n","        legend (bool): Whether to show the legend.\n","        legend_loc (str): Location of the legend.\n","        figsize (tuple): Size of the figure.\n","        linewidth (int): Width of the lines.\n","    \"\"\"\n","    # Set up the matplotlib figure and size\n","    plt.figure(figsize=figsize)\n","\n","    # Apply a Seaborn style\n","    sns.set(style=\"whitegrid\")\n","\n","    # Create the line plot\n","    sns.lineplot(data=data, x=x, y=y, hue=hue, palette=palette, style=hue if line_styles else None, \n","                 dashes=line_styles, markers=markers, linewidth=linewidth)\n","    \n","    # Customize gridlines\n","    if grid:\n","        plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n","    \n","    # Set the title and labels\n","    if title:\n","        plt.title(title, fontsize=16, fontweight='bold')\n","    if xlabel:\n","        plt.xlabel(xlabel, fontsize=14)\n","    if ylabel:\n","        plt.ylabel(ylabel, fontsize=14)\n","    plt.xticks(ticks=range(27), labels=labels, rotation=45, ha=\"right\", fontsize=12)\n","\n","    # Customize the legend\n","    if legend:\n","        plt.legend(title=hue, loc=legend_loc, title_fontsize='13', fontsize='11', frameon=True, framealpha=0.9)\n","\n","    # Improve the overall layout\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n","\n","import pandas as pd\n","\n","pattern_hook_names_filter = lambda name: name.endswith(\"hook_resid_pre\") or name.endswith(\"25.hook_resid_post\") or  name.endswith('ln_final.hook_scale')\n","\n","with torch.no_grad():\n","    _,cache = model.run_with_cache(toks,names_filter = pattern_hook_names_filter)\n","\n","toks = generation_dict[\"Vegetables\"][0].squeeze()\n","hypen_positions = torch.where(toks == hypen_tok_id)[0].to(\"cpu\")\n","break_positions = torch.where(toks == break_tok_id)[0].to(\"cpu\")\n","eot_positions = torch.where(toks == eot_tok_id)[0].to(\"cpu\")\n","filter_break_pos = [pos.item() for pos in break_positions if pos+1 in hypen_positions] + [eot_positions[-1].item()-1]\n"]},{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","all_logit_lens = []\n","for pos in filter_break_pos:\n","    accumulated_residual, labels = cache.accumulated_resid(\n","        incl_mid=False, pos_slice=pos, return_labels=True,apply_ln = True, \n","    )\n","\n","    dir = model.W_U[:,eot_tok_id].detach()-model.W_U[:,hypen_tok_id].detach()\n","    logit_lens = einops.einsum(dir, accumulated_residual,\"d_model,comps batch d_model -> comps batch \")\n","    all_logit_lens.append(logit_lens)\n","\n","data_dict = {}\n","data_dict[\"Layers\"] = list(range(model.cfg.n_layers+1))\n","for i,tensor in enumerate(all_logit_lens):\n","    data_dict[f\"Item {i}\"] =  tensor.cpu().reshape(-1).numpy()\n","\n","data = pd.DataFrame(data_dict)\n","                     \n","# Melt the data for seaborn compatibility\n","melted_data = pd.melt(data, id_vars=['Layers'], value_vars=list(data_dict.keys())[1:], \n","                      var_name='List Item', value_name='Value')\n","\n","plot_superposed_lineplots(melted_data, x='Layers', y='Value', hue='List Item', \n","                          title='Logit lens', xlabel='Layer', ylabel='Value',\n","                          palette='husl',  markers=True)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":false},"outputs":[],"source":["\n","all_logit_lens = []\n","for pos in hypen_positions+1:\n","    accumulated_residual, labels = cache.accumulated_resid(\n","        incl_mid=False, pos_slice=pos, return_labels=True,apply_ln = True\n","    )\n","\n","    dir = model.W_U[:,235248].detach()-model.W_U[:,break_tok_id].detach()\n","    logit_lens = einops.einsum(dir, accumulated_residual,\"d_model,comps batch d_model -> comps batch \")\n","    all_logit_lens.append(logit_lens)\n","\n","data_dict = {}\n","data_dict[\"Layers\"] = list(range(model.cfg.n_layers+1))\n","for i,tensor in enumerate(all_logit_lens):\n","    data_dict[f\"Item {i}\"] =  tensor.cpu().reshape(-1).numpy()\n","\n","data = pd.DataFrame(data_dict)\n","                     \n","# Melt the data for seaborn compatibility\n","melted_data = pd.melt(data, id_vars=['Layers'], value_vars=list(data_dict.keys())[1:], \n","                      var_name='List Item', value_name='Value')\n","\n","plot_superposed_lineplots(melted_data, x='Layers', y='Value', hue='List Item', \n","                          title='Logit lens', xlabel='Layer', ylabel='Value',\n","                          palette='husl',  markers=True)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## What are the most important componetnts\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### Attention Exploration\n"]},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false},"outputs":[],"source":["with torch.no_grad():\n","    _,cache = model.run_with_cache(toks)\n","dir = model.W_U[:,235248].detach()-model.W_U[:,break_tok_id].detach()\n","per_head_residual, labels = cache.stack_head_results(\n","    layer=-1, pos_slice=hypen_positions[-1]+1, return_labels=True\n",")\n","per_head_residual_per_layer = einops.rearrange(per_head_residual,\"(layer n_head) batch d_model-> layer n_head batch d_model\", layer = model.cfg.n_layers, n_head = model.cfg.n_heads)\n","per_head_layer_logit_diff = einops.einsum(per_head_residual_per_layer,dir.detach(),\"layer n_head batch d_model, d_model-> layer n_head batch\")[:,:,0].to(\"cpu\").detach()\n"]},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false},"outputs":[],"source":["np_labels = np.array(labels).reshape(26,8)\n","data = {f\"Layer {i}\":arr for i,arr in enumerate(per_head_layer_logit_diff)}\n","data[\"Head\"] = torch.tensor(list(range(8)))\n","df = pd.DataFrame(data)\n","df = df.set_index(\"Head\")\n","sns.heatmap(df)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### Naively get the top model components by lodit diff\n"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false},"outputs":[],"source":["toks[0,47]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","with torch.no_grad():\n","    _,cache = model.run_with_cache(toks)\n","resid_decomp, labels = cache.get_full_resid_decomposition(layer = -1, pos_slice = 48, apply_ln=True, return_labels = True,expand_neurons=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["label_index = []\n","color_values = []\n","for lab in labels:\n","    if lab.startswith(\"L\"):\n","        label_index.append(int(lab.split(\"L\")[1].split(\"H\")[0]))\n","        color_values.append(1)\n","    \n","    elif lab.endswith(\"out\"):\n","        label_index.append(int(lab.split(\"_\")[0]))\n","        color_values.append(2)\n","    else:\n","        label_index.append(-1)\n","        color_values.append(0)\n","color_map = {0: 'green', 1: 'blue', 2: 'red'}\n","colors = [color_map[val] for val in color_values]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["per_comp_logit_diff = einops.einsum(resid_decomp,dir.detach(),\" comp batch d_model, d_model->  comp batch\")[:,0].to(\"cpu\").detach()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["val,ind = per_comp_logit_diff.topk(k =10)\n","plt.figure(figsize=(10, 6))\n","plt.scatter(label_index, per_comp_logit_diff, color=colors)\n","# Identify the top 10 values\n","top_10_x = [label_index[i] for i in ind]\n","top_10_names = [labels[i] for i in ind]\n","\n","# Annotate the top 10 values\n","for i in range(10):\n","    plt.annotate(\n","        top_10_names[i],\n","        (top_10_x[i], val[i]),\n","        xytext=(5, 5),\n","        textcoords='offset points',\n","        arrowprops=dict(arrowstyle='->', color='red')\n","    )\n","\n","# Labels and title\n","plt.xlabel('Layers')\n","plt.ylabel('Logit Difference')\n","plt.title('Logit Lens for all the model components')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["# Ablation Experiments\n","\n","\n","\n","1. After the last element of the last item, the model usually outputs a blank space.\n","\n","Which Components can we zero ablate to make it output a \"\\n\" instead of \" \"\n","\n","\n","2. Which elements can we ablate in the last break to make the model output \"-\" instead of \"<end_of_turn>\"\n"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false},"outputs":[],"source":["\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import utils\n","from functools import partial\n","import tqdm\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**Attention Value Ablation**\n"]},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","def head_ablation_hook(\n","    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    pos: Int,\n","    head_to_ablate: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    value[:, pos, head_to_ablate, :] = 0.\n","    return value\n","\n","\n","\n","def get_all_v_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    logit_diff_mat = model.cfg.n_key_value_heads\n","    logit_diff_mat = np.zeros((model.cfg.n_key_value_heads, n_layers))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for head_to_ablate in tqdm.tqdm(range(model.cfg.n_key_value_heads)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(head_ablation_hook, pos=pos, head_to_ablate=head_to_ablate)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    utils.get_act_name(\"v\", layer_to_ablate), \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[head_to_ablate,layer_to_ablate] = ablated_logit_diff/clean_logit_diff\n","    return logit_diff_mat\n","\n","\n","\n","\n","pos = 46\n","logit_diff_mat = get_all_v_ablations(toks, pos)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false},"outputs":[],"source":["# Plotly heatmap\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Layers\", y=\"Heads\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n","                y=[f\"Head {i}\" for i in range(model.cfg.n_key_value_heads)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","def head_ablation_hook(\n","    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    pos: Int,\n","    head_to_ablate: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    value[:, pos, head_to_ablate, :] = 0\n","    return value\n","\n","\n","\n","def get_all_v_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    logit_diff_mat = np.zeros((model.cfg.n_heads, n_layers))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for head_to_ablate in tqdm.tqdm(range(model.cfg.n_heads)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(head_ablation_hook, pos=pos, head_to_ablate=head_to_ablate)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    utils.get_act_name(\"z\", layer_to_ablate), \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[head_to_ablate,layer_to_ablate] = ablated_logit_diff\n","    return logit_diff_mat\n","\n","\n","\n","\n","pos = 46\n","logit_diff_mat = get_all_v_ablations(toks, pos)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false},"outputs":[],"source":["\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Layers\", y=\"Heads\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n","                y=[f\"Head {i}\" for i in range(model.cfg.n_heads)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**Residual Stream Patching**\n"]},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def rs_ablation_hook(\n","    act: Float[torch.Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    pos: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    act[:,pos,:] = 0\n","    return act\n","\n","\n","\n","def get_all_resiudal_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    positions = list(range(33,pos-1))\n","    logit_diff_mat = np.zeros((n_layers, len(positions)))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for i,pos1 in tqdm.tqdm(enumerate(positions)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(rs_ablation_hook, pos=pos1)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    f\"blocks.{layer_to_ablate}.hook_resid_pre\", \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[layer_to_ablate,i] = ablated_logit_diff\n","    return logit_diff_mat\n","pos = 46\n","logit_diff_mat = get_all_resiudal_ablations(toks, pos)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false},"outputs":[],"source":["\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Positions\", y=\"Layers\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Position {i}\" for i in range(33,pos-1)],\n","                y=[f\"Layers {i}\" for i in range(model.cfg.n_layers)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**Logit Diff Patching hook_q**\n"]},{"cell_type":"code","execution_count":35,"metadata":{"collapsed":false},"outputs":[],"source":["cache[\"blocks.0.attn.hook_q\"].shape\n"]},{"cell_type":"code","execution_count":36,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def head_ablation_hook(\n","    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    pos: Int,\n","    head_to_ablate: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    value[:, pos, head_to_ablate, :] = 0\n","    return value\n","\n","\n","\n","def get_all_v_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    logit_diff_mat = np.zeros((model.cfg.n_heads, n_layers))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for head_to_ablate in tqdm.tqdm(range(model.cfg.n_heads)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(head_ablation_hook, pos=pos, head_to_ablate=head_to_ablate)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    utils.get_act_name(\"q\", layer_to_ablate), \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[head_to_ablate,layer_to_ablate] = ablated_logit_diff\n","    return logit_diff_mat\n","\n","\n","\n","\n","pos = 46\n","logit_diff_mat = get_all_v_ablations(toks, pos)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Layers\", y=\"Heads\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n","                y=[f\"Head {i}\" for i in range(model.cfg.n_heads)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**Patch key value**\n"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def head_ablation_hook(\n","    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    pos: Int,\n","    head_to_ablate: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    value[:, 30, head_to_ablate, :] = 0\n","    return value\n","\n","\n","\n","def get_all_v_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    logit_diff_mat = np.zeros((model.cfg.n_key_value_heads, n_layers))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for head_to_ablate in tqdm.tqdm(range(model.cfg.n_key_value_heads)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(head_ablation_hook, pos=pos, head_to_ablate=head_to_ablate)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    utils.get_act_name(\"k\", layer_to_ablate), \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[head_to_ablate,layer_to_ablate] = ablated_logit_diff\n","    return logit_diff_mat\n","\n","\n","\n","\n","pos = 46\n","logit_diff_mat = get_all_v_ablations(toks, pos)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false},"outputs":[],"source":["\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Layers\", y=\"Heads\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n","                y=[f\"Head {i}\" for i in range(model.cfg.n_key_value_heads)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**MLP output ablation**\n"]},{"cell_type":"code","execution_count":28,"metadata":{"collapsed":false},"outputs":[],"source":["def mlp_ablation_hook(\n","    act: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    pos: Int,\n",") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n","    act[:, pos,:] = 0\n","    return act\n","\n","\n","\n","\n","\n","\n","def get_all_mlp_ablations(toks, pos ):\n","    n_layers = model.cfg.n_layers\n","    positions = list(range(33,pos-1))\n","    logit_diff_mat = np.zeros((n_layers, len(positions)))\n","    original_logits = model(toks, return_type=\"logits\")\n","    clean_logit_diff = original_logits[:,pos,235248] - original_logits[:,pos,break_tok_id]  \n","    for i,pos1 in tqdm.tqdm(enumerate(positions)):\n","        for layer_to_ablate in range(n_layers):\n","            hook_func = partial(mlp_ablation_hook, pos=pos1)\n","            ablated_logits = model.run_with_hooks(\n","                toks, \n","                return_type=\"logits\", \n","                fwd_hooks=[(\n","                    f\"blocks.{layer_to_ablate}.hook_mlp_out\", \n","                    hook_func\n","                    )]\n","                )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[layer_to_ablate,i] = ablated_logit_diff\n","    return logit_diff_mat\n","\n","pos = 46\n","logit_diff_mat = get_all_mlp_ablations(toks, pos)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false},"outputs":[],"source":["\n","fig = px.imshow(logit_diff_mat, labels=dict(x=\"Positions\", y=\"Layers\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[f\"Position {i}\" for i in range(33,pos-1)],\n","                y=[f\"Layers {i}\" for i in range(model.cfg.n_layers)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["**Replacement Hook Residual Stream**\n"]},{"cell_type":"code","execution_count":125,"metadata":{"collapsed":false},"outputs":[],"source":["torch.cuda.empty_cache()\n","def get_resid(toks,positions):\n","    names_filter = lambda x: \"hook_resid_pre\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_resids = [torch.stack([cache[f\"blocks.{i}.hook_resid_pre\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_resids = torch.stack(all_resids).mean(1).mean(1)\n","    print(all_resids.shape)\n","    return all_resids\n","\n","def get_ln1_normalized(toks,positions):\n","    names_filter = lambda x: \"ln1.hook_normalized\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_ln1_normalized = [torch.stack([cache[f\"blocks.{i}.ln1.hook_normalized\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_ln1_normalized = torch.stack(all_ln1_normalized).mean(1).mean(1)\n","    print(all_ln1_normalized.shape)\n","    return all_ln1_normalized\n","\n","\n","\n","def get_mlp(toks,positions):\n","    names_filter = lambda x: \"hook_mlp_out\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_mlps = [torch.stack([cache[f\"blocks.{i}.hook_mlp_out\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_mlps = torch.stack(all_mlps).mean(1).mean(1)\n","    print(all_mlps.shape)\n","    return all_mlps\n","\n","def get_attn(toks,positions):\n","    names_filter = lambda x: \"hook_attn_out\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_attn = [torch.stack([cache[f\"blocks.{i}.hook_attn_out\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_attn = torch.stack(all_attn).mean(1).mean(1)\n","    print(all_attn.shape)\n","    return all_attn\n","\n","def get_attn_z(toks, positions):\n","    names_filter = lambda x: \"hook_z\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_attn = [torch.stack([cache[f\"blocks.{i}.attn.hook_z\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_attn = torch.stack(all_attn).mean(1).mean(1)\n","    print(all_attn.shape)\n","    return all_attn\n","\n","def get_attn_q(toks, positions):\n","    names_filter = lambda x: \"hook_q\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_attn = [torch.stack([cache[f\"blocks.{i}.attn.hook_q\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_attn = torch.stack(all_attn).mean(1).mean(1)\n","    print(all_attn.shape)\n","    return all_attn\n","\n","\n","def get_attn_k(toks, positions):\n","    names_filter = lambda x: \"hook_k\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_attn = [torch.stack([cache[f\"blocks.{i}.attn.hook_k\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_attn = torch.stack(all_attn).mean(1).mean(1)\n","    print(all_attn.shape)\n","    return all_attn\n","\n","def get_attn_v(toks, positions):\n","    names_filter = lambda x: \"hook_v\" in x\n","    with torch.no_grad():\n","        _,cache = model.run_with_cache(toks, names_filter = names_filter)\n","    all_attn = [torch.stack([cache[f\"blocks.{i}.attn.hook_v\"][:,p+1] for p in positions]) for i in range(26)]\n","    all_attn = torch.stack(all_attn).mean(1).mean(1)\n","    print(all_attn.shape)\n","    return all_attn\n","\n","\n","\n","hypen_positions = torch.where(toks[0] == hypen_tok_id)[0]\n","def get_all_mean_ablations(toks,pos):\n","    resid_mean = get_resid(toks, hypen_positions[:-1])\n","    ln1_normalized_mean = get_ln1_normalized(toks, hypen_positions[:-1])\n","    resid_mlp = get_mlp(toks, hypen_positions[:-1])\n","    attn_mean = get_attn(toks, hypen_positions[:-1])\n","    z_mean = get_attn_z(toks, hypen_positions[:-1])\n","    q_mean = get_attn_q(toks, hypen_positions[:-1])\n","    k_mean = get_attn_k(toks, hypen_positions[:-1])\n","    v_mean = get_attn_v(toks, hypen_positions[:-1])\n","\n","    def resid_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = resid_mean[layer].unsqueeze(0)\n","        return acts\n","    def ln1_normalized_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = ln1_normalized_mean[layer].unsqueeze(0)\n","        return acts\n","    def mlp_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = resid_mlp[layer].unsqueeze(0)\n","        return acts\n","    def attn_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = attn_mean[layer].unsqueeze(0)\n","        return acts\n","    def attn_z_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = z_mean[layer].unsqueeze(0)\n","        return acts\n","    def attn_q_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = q_mean[layer].unsqueeze(0)\n","        return acts\n","    def attn_k_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = k_mean[layer].unsqueeze(0)\n","        return acts\n","    def attn_v_replacement_hook(acts,hook,pos,layer):\n","        acts[:,pos,:] = v_mean[layer].unsqueeze(0)\n","        return acts\n","    logit_diff_mat = np.zeros((model.cfg.n_layers,8))\n","    n_layers = model.cfg.n_layers\n","\n","    for layer_to_ablate in range(n_layers):\n","        for comp_id,comp in enumerate([\"hook_resid_pre\",\"hook_mlp_out\",\"hook_attn_out\",\"attn.hook_z\",\"attn.hook_q\",\"attn.hook_k\",\"ln1.hook_normalized\",\"attn.hook_v\"]):\n","            if comp == \"hook_resid_pre\":\n","                hook_func = partial(resid_replacement_hook, pos=pos,layer = layer_to_ablate)\n","            elif comp == \"hook_mlp_out\":\n","                hook_func = partial(mlp_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"hook_attn_out\":\n","                hook_func = partial(attn_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"attn.hook_z\":\n","                hook_func = partial(attn_z_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"attn.hook_q\":\n","                hook_func = partial(attn_q_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"attn.hook_k\":\n","                hook_func = partial(attn_k_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"attn.hook_v\":\n","                hook_func = partial(attn_v_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            elif comp == \"ln1.hook_normalized\":\n","                hook_func = partial(ln1_normalized_replacement_hook,pos = pos, layer = layer_to_ablate) \n","            with torch.no_grad():\n","                ablated_logits = model.run_with_hooks(\n","                    toks, \n","                    return_type=\"logits\", \n","                    fwd_hooks=[(\n","                        f\"blocks.{layer_to_ablate}.{comp}\", \n","                        hook_func\n","                        )]\n","                    )\n","            ablated_logit_diff = ablated_logits[:,pos,235248] - ablated_logits[:,pos,break_tok_id]  \n","            logit_diff_mat[layer_to_ablate,comp_id] = ablated_logit_diff.cpu().numpy()\n","    return logit_diff_mat\n","    \n","\n","logit_diff_mat = get_all_mean_ablations(toks, 46)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":119,"metadata":{"collapsed":false},"outputs":[],"source":["cache[\"blocks.0.ln1.hook_scale\"].shape\n"]},{"cell_type":"code","execution_count":126,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","comps = [\"hook_resid_pre\",\"hook_mlp_out\",\"hook_attn_out\",\"attn.hook_z\",\"attn.hook_q\",\"attn.hook_k\",\"ln1.hook_normalized\",\"attn.hook_v\"]\n","fig = px.imshow(logit_diff_mat.T, labels=dict(x=\"Positions\", y=\"Layers\", color=\"Logit Difference\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                y=[comp for comp in comps],\n","                x=[f\"Layers {i}\" for i in range(model.cfg.n_layers)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["# Viusalize attention patterns\n"]},{"cell_type":"code","execution_count":127,"metadata":{"collapsed":false},"outputs":[],"source":["pattern_hook_names_filter = lambda name: \"pattern\" in name\n","with torch.no_grad():\n","    _,cache = model.run_with_cache(toks,names_filter = pattern_hook_names_filter)\n","\n","\n","cache = cache.to(\"cpu\")\n","\n"]},{"cell_type":"code","execution_count":128,"metadata":{"collapsed":false},"outputs":[],"source":["pos = 46\n","all_patterns_pos = torch.stack([cache[f\"blocks.{i}.attn.hook_pattern\"][0,:,pos] for i in range(25)])\n","all_patterns_max = all_patterns_pos.max(dim = 1).values\n","\n","str_tokens = model.to_str_tokens(toks)\n","unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(str_tokens)] \n","fig = px.imshow(all_patterns_max, labels=dict(x=\"Toks\", y=\"Layers\", color=\"Attention Pattern\"),\n","                title=f\"Logit Difference Ablations for Position {pos}\",\n","                x=[i for i in unique_tokens],\n","                y=[f\"Layer {i}\" for i in range(25)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":126,"metadata":{"collapsed":false},"outputs":[],"source":["pos = 46\n","str_tokens = model.to_str_tokens(toks)\n","unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(str_tokens)] \n","pattern = cache[\"blocks.21.attn.hook_pattern\"][0,:,pos]\n","fig = px.imshow(pattern, labels=dict(x=\"Toks\", y=\"Heads\", color=\"Attention Pattern\"),\n","                title=f\"Attention Patterns\",\n","                x=[i for i in unique_tokens],\n","                y=[f\"Head {i}\" for i in range(8)],\n","                color_continuous_scale='Viridis')\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## SAEs\n"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false},"outputs":[],"source":["from attribution_utils import calculate_feature_attribution\n","from torch.nn.functional import log_softmax\n","from gemma_utils import get_all_string_min_l0_resid_gemma\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import utils\n","from functools import partial\n","import tqdm\n","from sae_lens import HookedSAETransformer, SAE, SAEConfig\n","from gemma_utils import get_gemma_2_config, gemma_2_sae_loader\n","import numpy as np\n","import torch\n","import tqdm\n","import einops\n","import re\n","from jaxtyping import Int, Float\n","from typing import List, Optional, Any\n","from torch import Tensor\n","import json\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","import random\n","from transformer_lens.utils import get_act_name\n","from IPython.display import display, HTML\n","import plotly.express as px\n"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false},"outputs":[],"source":["\n","model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b-it\")\n","generation_dict = torch.load(\"gemma2_generation_dict.pt\")\n","toks = generation_dict[\"Vegetables\"][0]\n","\n","hypen_tok_id = 235290\n","break_tok_id = 108\n","eot_tok_id = 107\n","blanck_tok_id = 235248\n","hypen_positions = torch.where(toks[0] == hypen_tok_id)[0]\n","break_positions = torch.where(toks[0] == break_tok_id)[0]\n","eot_positions = torch.where(toks[0] == eot_tok_id)[0]\n","filter_break_pos = [pos.item() for pos in break_positions if pos+1 in hypen_positions]\n"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false},"outputs":[],"source":["pos = 46\n","toks[:,pos]\n"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def metric_fn(logits: torch.Tensor, pos:int = 46) -> torch.Tensor:\n","    return logits[0,pos,235248] - logits[0,pos,break_tok_id]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false},"outputs":[],"source":["full_strings = get_all_string_min_l0_resid_gemma()\n","layer = 20\n","saes_dict = {}\n","with torch.no_grad():\n","    repo_id = \"google/gemma-scope-2b-pt-res\"\n","    folder_name = full_strings[layer]\n","    config = get_gemma_2_config(repo_id, folder_name)\n","    cfg, state_dict, log_spar = gemma_2_sae_loader(repo_id, folder_name)\n","    sae_cfg = SAEConfig.from_dict(cfg)\n","    sae = SAE(sae_cfg)\n","    sae.load_state_dict(state_dict)\n","    sae.to(\"cuda:0\")\n","    sae.use_error_term = True\n","\n","    saes_dict[sae.cfg.hook_name] = sae\n"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[],"source":["import pandas as pd\n","import plotly.express as px\n"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false},"outputs":[],"source":["\n","feature_attribution_df = calculate_feature_attribution(\n","    model = model,\n","    input = toks,\n","    metric_fn = metric_fn,\n","    include_saes=saes_dict,\n","    include_error_term=True,\n","    return_logits=True,\n",")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false},"outputs":[],"source":["def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n","    \"\"\"\n","    Convert a sparse tensor to a long format pandas DataFrame.\n","    \"\"\"\n","    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n","    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n","    df_long.columns = [\"feature\", \"attribution\"]\n","    df_long_nonzero = df_long[df_long['attribution'] != 0]\n","    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n","    return df_long_nonzero\n","\n","df_long_nonzero = convert_sparse_feature_to_long_df(feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0])\n","df_long_nonzero.sort_values(\"attribution\", ascending=False)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Parallel Cordinates Plot \n"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[],"source":["positions = [pos-1 for pos in all_tok2_pos+[tok_pos]]\n","attr_df_pos = df_long_nonzero[df_long_nonzero['position'].isin(positions)]\n","attr_df_pos = attr_df_pos.pivot(index = \"feature\", columns = \"position\", values = \"attribution\").reset_index()\n","mat = df.iloc[:,2:].to_numpy()\n","sort = mat.argsort(axis = 0)\n","dataframe = pd.DataFrame.from_records(sort).reset_index()\n","dataframe[\"feature\"] = df['feature'].tolist()\n","df = attr_df_pos.dropna(axis = 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["\n","mean = mat.mean(axis=1)\n","\n","# New normalization with dramatic difference\n","power = 2  # Adjust this value to increase or decrease the effect\n","colors = (mean - mean.min()) / (mean.max() - mean.min())\n","colors = np.power(colors, power)\n","top_10_indices = np.argsort(mean)[-10:][::-1]\n","\n","plt.figure(figsize=(10, 6))\n","\n","\n","# Plot each feature\n","for i in range(len(dataframe)):\n","    y = dataframe.iloc[i, 1:-1].values\n","    x = range(len(y))\n","    color = str(1 - colors[i])  # Invert the color value (darker = higher value)\n","    line = plt.plot(x, y, color=color, linewidth=2, alpha=0.7)\n","    \n","    # Annotate top 10 lines\n","    if i in top_10_indices:\n","        plt.annotate(dataframe['feature'][i], \n","                     xy=(len(x)-1, y[-1]), \n","                     xytext=(5, 0), \n","                     textcoords='offset points',\n","                     ha='left', \n","                     va='center', \n","                     fontsize=8,\n","                     color=color,\n","                     fontweight='bold')\n","# Customize the plot\n","plt.title('Parallel Coordinates Plot')\n","plt.xlabel('Positions')\n","plt.ylabel('Rank')\n","plt.xticks(range(len(dataframe.columns[1:-1])), dataframe.columns[1:-1], rotation=45)\n","plt.grid(False)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["feat_dir = sae.W_dec[4411,:].detach()\n","\n","f_tok_val,f_tok_ind = (feat_dir@model.W_U.detach()).topk(k = 10)\n","highest_logtis_dict = {}\n","highest_logtis_dict[\"Token\"] = []\n","highest_logtis_dict[\"Value\"] = []\n","for v,t in zip(f_tok_val,f_tok_ind):\n","    highest_logtis_dict[\"Token\"] += [model.to_string(t)]\n","    highest_logtis_dict[\"Value\"] += [v.to(\"cpu\").item()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["\n","df_highest_tok = pd.DataFrame.from_dict(highest_logtis_dict).reset_index().set_index(\"Token\")\n","\n","df_highest_tok\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["model.to_string(tokens[0,tok_pos])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["tokens = model.to_str_tokens(string, prepend_bos=False)[:tok_pos]\n","unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(tokens)]\n","\n","px.bar(x = unique_tokens,\n","       y = feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0].sum(-1).detach().cpu().numpy()[:tok_pos])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["# Just for the feature 4411\n","\n","\n","px.bar(x = unique_tokens,\n","       y = feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0][:,4411].detach().cpu().numpy()[:tok_pos],title =\"Attribution for feature 4411\")\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### Get the top features in the last position\n","\n","Promote end of list [ 4411, 13491,  5325,  1777,  6004, 11942,  9369,  1000,  4384,  3855]\n","\n","\n","Promote continuation of list[10529,   152, 12523,  8323,   492, 10548,  5169, 14540,   561,  7368]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["val, ind = feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0,tok_pos-1].topk(k = 10,dim = -1)\n","ind = ind.tolist()\n","ind\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":["from functools import partial\n","\n","def prompt_with_ablation(model, sae, prompt, ablation_features):\n","    \n","    def ablate_feature_hook(feature_activations, hook, feature_ids, position = None):\n","    \n","        if position is None:\n","            feature_activations[:,:,feature_ids] = 0\n","        else:\n","            feature_activations[:,position,feature_ids] = 30\n","\n","        return feature_activations\n","        \n","    ablation_hook = partial(ablate_feature_hook, feature_ids = ablation_features, position = 46)\n","    \n","    model.add_sae(sae)\n","    hook_point = sae.cfg.hook_name + '.hook_sae_acts_post'\n","    model.add_hook(hook_point, ablation_hook, \"fwd\")\n","    \n","    \n","    logits = model(prompt)\n","\n","\n","    \n","    model.reset_saes()\n","    model.reset_hooks()\n","    return logits\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Zero ablation experiment\n","**How do the llogits change when we ablate each one of the features**\n"]},{"cell_type":"code","execution_count":213,"metadata":{"collapsed":false},"outputs":[],"source":["\n","\n","tokens = model.to_tokens(string,prepend_bos=False)[0][:tok_pos]\n","with torch.no_grad():\n","    original_logits = model(tokens)\n","    original_logit_diff = original_logits[0,-1,3119] - original_logits[0,-1,2577]\n","\n","original_logit_diff\n"]},{"cell_type":"code","execution_count":272,"metadata":{"collapsed":false},"outputs":[],"source":["all_logit_diff = []\n","for feat in ind:\n","#for i in range(0,len(ind)):\n","    model.reset_hooks(including_permanent=True)\n","    logits = prompt_with_ablation(model, sae, tokens,feat)\n","    logit_diff = logits[0,-1,3119] - logits[0,-1,2577]\n","    all_logit_diff.append(logit_diff.cpu().item()) \n","\n","\n","print(all_logit_diff)\n"]},{"cell_type":"code","execution_count":248,"metadata":{"collapsed":false},"outputs":[],"source":["feature_effect = defaultdict(list) \n","for i in range(len(all_logit_diff)):\n","    feature_effect[\"Feature\"] += [ind[i]]\n","    feature_effect[\"Zero Ablation Logit Diff \"] += [all_logit_diff[i]]\n","\n","df_feature_effect = pd.DataFrame.from_dict(feature_effect)\n","df_feature_effect\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### Generate with intervention\n","\n","Set the feature 4411 in the last position to 10\n"]},{"cell_type":"code","execution_count":256,"metadata":{"collapsed":false},"outputs":[],"source":["feat_4411_acts = feature_attribution_df.sae_feature_activations['blocks.22.hook_resid_post'][0,:,4411]\n","filterd_feat_4411_acts = [feat_4411_acts[i] for i in positions]\n"]},{"cell_type":"code","execution_count":257,"metadata":{"collapsed":false},"outputs":[],"source":["filterd_feat_4411_acts\n"]},{"cell_type":"code","execution_count":285,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def steering_hook(feature_activations, hook, feature_ids=4411, position = None,val= filterd_feat_4411_acts[2]):\n","\n","    if feature_activations.shape[1]>1:\n","        # Inital batch of activations\n","        if position is None:\n","            feature_activations[:,:,feature_ids] = 0\n","        else:\n","            if type(feature_ids)==list:\n","                for f in feature_ids:\n","                    feature_activations[:,position,f] = 0 \n","            else:    \n","                print(feature_activations.shape)\n","                feature_activations[:,position,feature_ids] = val\n","\n","        return feature_activations\n","    else:\n","        return feature_activations\n","\n","string = \"<bos><body><h1>List of My Brother's Favourite Cities</h1><ul><li>Bangkok, Thailand</li><li>London, England</li><li>Paris, France</li><li>Melbourne, Australia</li><li>Toronto, Canada</li><li>New York, USA</li></ul></body>\\n<eos>\"\n","tokens = model.to_tokens(string, prepend_bos=False)[:,:tok_pos]\n","model.add_sae(sae)\n","with model.hooks(fwd_hooks=[(sae.cfg.hook_name+\".hook_sae_acts_post\", steering_hook)]):\n","    output = model.generate(\n","        tokens,\n","        max_new_tokens=100,\n","        temperature=0.7,\n","        top_p=0.9,\n","        stop_at_eos =  True,\n","        prepend_bos = sae.cfg.prepend_bos,\n","    )\n"]},{"cell_type":"code","execution_count":286,"metadata":{"collapsed":false},"outputs":[],"source":["print(model.to_string(output))\n","visualize_html(model.to_string(output)[0])\n"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["### DFA\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false},"outputs":[],"source":["# What are the features that most contribute to the fireing of feature 4411\n","with torch.no_grad():\n","    logits, cache = model.run_with_cache(tokens)\n"]},{"cell_type":"code","execution_count":155,"metadata":{"collapsed":false},"outputs":[],"source":["\n","pattern = cache[get_act_name(\"pattern\",22)]\n","out = cache[\"blocks.22.hook_attn_out\"]\n","stacked_values = cache['blocks.22.attn.hook_v']\n","v_cat = torch.repeat_interleave(stacked_values, dim=2, repeats=2)\n","v_cat = einops.rearrange(v_cat, \"batch src_pos n_heads  d_head -> batch src_pos (n_heads d_head) \",d_head = model.cfg.d_head,n_heads = model.cfg.n_heads)\n"]},{"cell_type":"code","execution_count":204,"metadata":{"collapsed":false},"outputs":[],"source":["\n","attn_weight = einops.repeat(pattern, \"batch n_heads dest src -> batch dest src (n_heads d_head)\",d_head = model.cfg.d_head, n_heads = model.cfg.n_heads)\n","decompose_z_cat = attn_weight*v_cat.unsqueeze(0)\n","W_O = model.state_dict()[\"blocks.18.attn.W_O\"].detach()\n","W_O_conc = einops.rearrange(W_O,\"n_heads d_head d_model -> (n_heads d_head) d_model\") \n","decompose_out = einops.einsum(W_O_conc,decompose_z_cat,\"d_attn d_model, batch dest src d_attn -> batch d_model dest src\")\n","scale = cache['blocks.22.ln2.hook_scale']\n","resid_rms = (decompose_out**2).mean(dim = 1,keepdim = True).sqrt()\n","decompose_out_normalized =  (decompose_out/resid_rms)*scale\n","dir_decompose_out = einops.einsum(decompose_out_normalized, dir, \"batch d_model dest src, d_model -> batch dest src\")\n"]},{"cell_type":"code","execution_count":205,"metadata":{"collapsed":false},"outputs":[],"source":["sns.heatmap(dir_decompose_out[0].cpu())\n","plt.show()\n"]},{"cell_type":"code","execution_count":211,"metadata":{"collapsed":false},"outputs":[],"source":["\n","tokens = model.to_str_tokens(string, prepend_bos=False)[:tok_pos]\n","unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(tokens)]\n","\n","px.bar(x = unique_tokens,\n","       y =dir_decompose_out[0,-1].cpu().numpy()) \n"]},{"cell_type":"code","execution_count":208,"metadata":{"collapsed":false},"outputs":[],"source":["dir_decompose_out[0,-1]\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
