{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_lens import HookedSAETransformer, SAE, SAEConfig\n",
    "from gemma_utils import get_gemma_2_config, gemma_2_sae_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import einops\n",
    "import re\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Any\n",
    "from torch import Tensor\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from transformer_lens.utils import get_act_name\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce1c81aeb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5114c64c8504c5f8d69ab1085f89ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbaf1ae84cf4ac0ad861fb8a09a42a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<body><h1>List of My Brother's Favourite Cities</h1><ul><li>Bangkok, Thailand</li><li>London, England</li><li>Paris, France</li><li>Melbourne, Australia</li><li>Toronto, Canada</li><li>New York, USA</li></ul></body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"<body><h1>List of My Brother's Favourite Cities</h1><ul><li>\"\"\"\n",
    "tokens = model.to_tokens(text)\n",
    "out = model.generate(\n",
    "    tokens,\n",
    "    max_new_tokens = 100,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    stop_at_eos=True,\n",
    "    )\n",
    "\n",
    "visualize_html(model.to_string(out[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_html(text):\n",
    "    text = text.replace(\"<eos>\",\"\")\n",
    "    text = text.replace(\"<bos>\",\"\")\n",
    "    display(HTML(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"<bos><body><h1>List of My Brother's Favourite Cities</h1><ul><li>Bangkok, Thailand</li><li>London, England</li><li>Paris, France</li><li>Melbourne, Australia</li><li>Toronto, Canada</li><li>New York, USA</li></ul></body>\\n<eos>\"\n",
    "tokens = model.to_tokens(string, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point and with the changes that we have made to the setup to reliably produce list that don't spiral out of control, we ask ourselves how can the model decide to add another elemnt or not.\n",
    "\n",
    "\n",
    "Taking into account the moel's tokenization:\n",
    "\n",
    "- `'<ul>' '<', 'ul', '>'`\n",
    "- `'><li>' '><', 'li', '>'`\n",
    "- `'</li>' '</', 'li', '>'`\n",
    "- `'><ul>' '></', 'ul', '>'`\n",
    "\n",
    "The model will continue the list while remaining html consistent if it uses the token `><` instead of the token `></`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_id = model.to_tokens(\"></\", prepend_bos=False)[0]\n",
    "\n",
    "tok_id2 = model.to_tokens(\"><\", prepend_bos=False)[0]\n",
    "tok_pos = torch.where(out[0] == tok_id)[0][0].tolist()\n",
    "all_tok2_pos = torch.where(out[0] == tok_id2)[0][1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7107], device='cuda:0')\n",
      "tensor([-2.0631], device='cuda:0')\n",
      "tensor([-1.3211], device='cuda:0')\n",
      "tensor([-1.1420], device='cuda:0')\n",
      "tensor([-0.5403], device='cuda:0')\n",
      "tensor([1.1881], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for elem in all_tok2_pos + [tok_pos]:\n",
    "    l = logits[0,elem]\n",
    "    print(l[tok_id]-l[tok_id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-3.6416e-03, -6.0267e+00, -8.7354e+00, -8.9372e+00, -9.0377e+00],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([601, 515, 561, 727, 913], device='cuda:0'))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l.log_softmax(dim = -1)).topk(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
